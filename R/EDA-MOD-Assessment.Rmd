---
title: "EDA-MOD-Assessment"
author: Adriel Martinez (adriel.martinez@mail.utoronto.ca)
output: html_notebook
---

NOTE: If you want to go through this notebook, clear all the code output. Click the gear icon beside preview and click _Clear All Output_


## How to use Rmarkdown
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
plot(cars)
```

## Some prep

We will be using a couple of libraries.

`ggplot2` is a plotting system that is similar to python's `matplotlib` and to R's native plotting system. This library will be used to display the various
plots in the R markdown file.

```{r message=FALSE, warning=FALSE}
if (!require(ggplot2, quietly=TRUE)) {
  install.packages("ggplot2")
  library(ggplot2)
}
```

`GGally` contains various additions to the `ggplot2` ecosystem. For the purpose of this demonstration, only one function (`ggpairs`) is used.

```{r}

if (!require(GGally, quietly=TRUE)) {
  install.packages("GGally")
  library(GGally)
}
```

Some cost functions will be used

```{r}
sumOfSquaresCost <- function(x, y, theta) {
  X <- matrix(c(1), ncol=length(x))
  X <- rbind(X, x)
  return(sum((y - (t(theta) %*% X)) ^ 2))
}

sumOfSquaresGrad <- function(x, y, theta) {
  X <- matrix(c(1), ncol=length(x))
  X <- rbind(X, x)
  tmp <- y - (t(theta) %*% X)
  rowWise <- sweep(X,MARGIN=2, tmp,`*`)
  return(-2 * matrix(rowSums(rowWise)))
}

sigmoidFunction <- function(x) {
  return(1/(1 + exp(-x)))
}

logisticCost <- function(x, y, theta) {
  X <- matrix(c(1), ncol=length(x))
  X <- rbind(X, x)
  h <- sapply(t(theta) %*% X, sigmoidFunction)
  totalCost <- 0
  for (i in 1:length(h)) {
    elementCost <- -(y[i] * log(h[i])) - ((1 - y) * log(1 - h[i]))
    totalCost <- totalCost + elementCost
  }
  return(totalCost)
}

logisticCostGrad <- function(x, y, theta) {
  X <- matrix(c(1), ncol=length(x))
  X <- rbind(X, x)
  h <- sapply(t(theta) %*% X, sigmoidFunction)
  tmp <- h - y
  rowWise <- sweep(X,MARGIN=2, tmp,`*`)
  return(matrix(rowSums(rowWise)))
}

polySumOfSquaresCost <- function(x, y, genFunction) {
  return(sum((y - sapply(x, genFunction)) ^ 2))
}
```

I guess gradient descent will be used as well.

```{r}
gradDescent <- function(x, y, theta, costFun, costGrad, alpha, maxIter, verbose=FALSE) {
  cost <- c(length=maxIter + 1)
  cost[1] <- costFun(x, y, theta)
  iter <- 0
  while (iter < maxIter) {
    theta <- theta - (alpha * costGrad(x, y, theta))
    iterCost <- costFun(x, y, theta)
    cost[iter + 2] <- iterCost
    if (verbose) {
      print(sprintf("Iter: %f/%f, Cost: %f", iter + 1, maxIter, iterCost))
    }
    iter <- iter + 1
  }
  return(list("theta"=theta, "cost"=cost))
}
```

## Linear Data Regression

### Generate some data

We will be generating data around the line y = 22x - 35

```{r}
# Store the coefficients in a theta vector
linearTheta <- matrix(c(-35, 22))
dataSize <- 2000
# Don't forget that seed
set.seed(123)
# Generate the x values between -20 and 20
linearX <- runif(dataSize, min=-20, max=20)
# To generate y, we need to append a row of 1's for the intercept
linearM <- matrix(c(1), ncol=length(linearX))
linearM <- rbind(linearM, linearX)
# The actual generation. Do matrix multiplication to generate the line values then add normal noise
linearY <- c(t(linearTheta) %*% linearM + rnorm(length(linearX), mean=10, sd=100))
```

Let's see the generated data.

```{r}
# Store in a data frame so we can plot it
linearDf <- data.frame(x=linearX, y=linearY)
linearPlot <- ggplot(linearDf, aes(x, y)) +
  geom_point(colour="dodgerblue",alpha=0.75) +
  geom_abline(aes(colour="green", intercept=linearTheta[1], slope=linearTheta[2]), alpha=0.8, size=1) +
  scale_color_identity(labels=c("True Line"), guide="legend") +
  ggtitle("Synthetic Data (Linear Data)") +
  labs(y="The Y value", x = "The X feature")
linearPlot
```

### The actual Linear Regression

Fitting a regression line is as _easy_ as running `lm()`.

```{r}
linearDataRegression <- lm(linearY ~ linearX)
linearDataRegression
```

Pretty close to the true line.

```{r message=FALSE, warning=FALSE}
# Add the regression line to the original plot
linearPlot <- linearPlot +
  geom_abline(aes(colour="black", intercept=linearDataRegression$coefficients[1], slope=linearDataRegression$coefficients[2]), alpha=0.8, size=1) +
  scale_color_identity(labels=c("Regression Line", "True Line"), guide="legend")
linearPlot
```

Let's try using gradient descent. The code is available in the __Some Prep__ section.

```{r}
# Start with an arbitrary theta. Gradient descent will lead to the optimum theta.
initTheta <- matrix(c(1, 2))
gradResults <- gradDescent(linearX, linearY, initTheta, sumOfSquaresCost, sumOfSquaresGrad, 3E-9, 5000)
gradResults$theta
```

Again pretty close. Plot it as well.

```{r message=FALSE, warning=FALSE}
# Add the gradient descent line to the original plot
linearPlot <- linearPlot +
  geom_abline(aes(colour="red", intercept=gradResults$theta[1], slope=gradResults$theta[2]), alpha=0.8, size=1) +
  scale_color_identity(labels=c("Regression Line", "Real Line", "Gradient Descent"), guide="legend")
linearPlot
```

Let's see those learning (cost) curves.

```{r}
gradDf <- data.frame(iter=1:length(gradResults$cost), cost=gradResults$cost)
costP <- ggplot(gradDf, aes(iter, cost)) +
  geom_point(colour="red",alpha=0.75, shape=42) +
  ggtitle("Gradient Descent: Cost vs Iterations") +
  labs(y="Sum of Squares", x = "Iterations")
costP
```
 
That's a smooth descent!

### Some metrics analysis

Let's look at the sum of squares error and mean squared error.

```{r}
# Real line
sumOfSquaresCost(linearX, linearY, linearTheta)
sumOfSquaresCost(linearX, linearY, linearTheta) / length(linearX)
# Built in R lm (Maximum likelihood estimator?)
mleTheta <- matrix(c(linearDataRegression$coefficients))
sumOfSquaresCost(linearX, linearY, mleTheta)
sumOfSquaresCost(linearX, linearY, mleTheta) / length(linearX)
# Gradient Descent
sumOfSquaresCost(linearX, linearY, gradResults$theta)
sumOfSquaresCost(linearX, linearY, gradResults$theta) / length(linearX)
```

The R lm algorithm did better than the real line. That's the power of randomness!

R includes some analysis in the regression model.

```{r}
summary(linearDataRegression)
```

### What if we didn't know that the data was linear?

We can use the AIC and BIC of fit models to determine how well a model is fit (loosely based on https://www.r-bloggers.com/aic-bic-vs-crossvalidation/)

Iterate through 1 (purely linear) to 10 polynomial degrees.

```{r}
# Iterate through and perform regression with differen polynomial degrees.
linearDegreesToExplore <- 10
linearAICResults <- vector(mode = "numeric", length=linearDegreesToExplore)
linearBICResults <- vector(mode = "numeric", length=linearDegreesToExplore)
for (degree in 1:linearDegreesToExplore) {
  regressionModel <- lm(linearY ~ poly(linearX, degree = degree))
  linearAICResults[degree] <- AIC(regressionModel)
  linearBICResults[degree] <- BIC(regressionModel)
}
```


```{r}
# Store in a dataframe to plot out
linearDegreesDF <- data.frame(degrees = 1:linearDegreesToExplore, AICResults = linearAICResults, BICResults = linearBICResults)
linearDegreesP <- ggplot(linearDegreesDF, aes(x=degrees)) +
  geom_line(aes(y=AICResults, color="AICResults")) +
  geom_line(aes(y=BICResults, color="BICResults")) +
  scale_x_continuous(breaks = 1:linearDegreesToExplore) + 
  ggtitle("Linear Data: Polynomial Degree vs AIC & BIC") +
  labs(y="AIC/BIC", x = "Polynomial Degree") +
  geom_segment(aes(x=1, y=24120, xend=1, yend=24110),
               arrow = arrow(length = unit(0.3, "cm"),
                             angle=20, type="closed"))
linearDegreesP
```

For well fit models, a lower AIC or BIC is prefered. As the degree increases, both the AIC and the BIC get larger. AIC and BIC penalizes overfitting so the best model was clearly just linear (1 degree).

## Nonlinear Data Regression

### Generate some data

This time, we'll generate data points for y = 2x^3 - x^2^ + 3x - 33

Let's generate a closure for generating the appropriate y values.

```{r}
genYFunction <- function(t, degreesOfPolynomial) {
  genY <- function(x) {
    total <- 0
    currDegree <- 0
    total <- 0
    for (i in 0:degreesOfPolynomial) {
      currAmount <- t[i + 1]  * (x ^ i)
      total <- total + currAmount
    }
    return(total)
  }
  return(genY)
}

```

Like before, let's generate some data.

```{r}
polyTheta <- matrix(c(-33, 3, 1, 2))
dataSize <- 1000
# Another. 10 because class starts at 10.
set.seed(10)
polyX <- runif(dataSize, min=-20, max=20)
realFunction <- genYFunction(polyTheta, 3)
polyY <- sapply(polyX, realFunction) + rnorm(length(polyX), mean=32, sd=3000)
```

It should look fine? Let's plot it.

```{r}
# Store in df to plot
polyDf <- data.frame(x=polyX, y=polyY)
polyP <- ggplot(polyDf, aes(x, y)) +
  geom_point(colour="dodgerblue",alpha=0.75) +
  stat_function(fun = realFunction, aes(colour = "black")) +
  scale_color_identity(labels=c("True Line"), guide="legend") +
  ggtitle("Synthetic Data (Polynomial)") +
  labs(y="The Y value", x = "The X feature")
polyP
```

### Regression

Now we regress. We still use `lm()`

```{r}
# While we still use lm(), the parameters are different now
nonlinReg <- lm(polyY ~ polyX + I(polyX^2) + I(polyX^3))
nonlinReg
```

```{r message=FALSE, warning=FALSE}
polyFitFunction <- genYFunction(nonlinReg$coefficients, length(nonlinReg$coefficients) - 1)
polyP <- polyP +
  stat_function(fun = polyFitFunction, aes(colour = "red")) +
  scale_color_identity(labels=c("True Line", "Poly Regression Line"), guide="legend")
polyP
```

Pretty close? What if we thought the data was linear?

```{r}
linReg <- lm(polyY ~ polyX)
linReg
```

```{r message=FALSE, warning=FALSE}
polyP <- polyP +
  geom_abline(aes(colour="green", intercept=linReg$coefficients[1], slope=linReg$coefficients[2]), alpha=0.8, size=1) +
  scale_color_identity(labels=c("True Line", "Lin Regression Line", "Poly Regression Line"), guide="legend")
polyP
```

Well that doesn't look right.

### Some metrics analysis

Let's look at sum of squares and mean squared error.

```{r}
# Real line
polySumOfSquaresCost(polyX, polyY, realFunction)
polySumOfSquaresCost(polyX, polyY, realFunction) / length(polyX)
# Polynomial regression
polySumOfSquaresCost(polyX, polyY, polyFitFunction)
polySumOfSquaresCost(polyX, polyY, polyFitFunction) / length(polyX)
# Linear regression
mleTheta <- matrix(c(linReg$coefficients))
sumOfSquaresCost(polyX, polyY, mleTheta)
sumOfSquaresCost(polyX, polyY, mleTheta) / length(polyX)
```

This time, the estimated polynomial line didn't do as good as the real line. The linear line was not even close...

```{r}
summary(nonlinReg)
summary(linReg)
```

### What if we didn't know which polynomial degree to use?

Again, we do the AIC and BIC analysis.

```{r}
# Iterate through and perform regression with differen polynomial degrees.
polyDegreesToExplore <- 10
polyAICResults <- vector(mode = "numeric", length=polyDegreesToExplore)
polyBICResults <- vector(mode = "numeric", length=polyDegreesToExplore)
for (degree in 1:polyDegreesToExplore) {
  regressionModel <- lm(polyY ~ poly(polyX, degree = degree))
  polyAICResults[degree] <- AIC(regressionModel)
  polyBICResults[degree] <- BIC(regressionModel)
}
```


```{r}
# Store in a dataframe to plot out
polyDegreesDF <- data.frame(degrees = 1:polyDegreesToExplore, AICResults = polyAICResults, BICResults = polyBICResults)
polyDegreesP <- ggplot(polyDegreesDF, aes(x=degrees)) +
  geom_line(aes(y=AICResults, color="AICResults")) +
  geom_line(aes(y=BICResults, color="BICResults")) +
  scale_x_continuous(breaks = 1:polyDegreesToExplore) + 
  ggtitle("Polynomial Data: Polynomial Degree vs AIC & BIC") +
  labs(y="AIC/BIC", x = "Polynomial Degree") +
  geom_segment(aes(x=3, y=19200, xend=3, yend=19100),
               arrow = arrow(length = unit(0.3, "cm"),
                             angle=20, type="closed"))
polyDegreesP
```

The data that was generate had three polynomial degrees which aligns with the AIC/BIC graph.

### Bonus Round: Regression on higher degree polynomial data.

Choose a random number between 5 and 10 (inclusive) and that will be how many polynomial degrees will be used to generate the data.

```{r}
# 85 because that's a 4.0
set.seed(85)
hiddenDegree <- sample(6:10, 1)
# Generate a theta. One extra for the intercept.
hiddenTheta <- matrix(sample(1:20, hiddenDegree + 1, replace=T))
# And generate data again
dataSize <- 1000
hiddenX <- runif(dataSize, min=-5, max=5)
hiddenFunction <- genYFunction(hiddenTheta, hiddenDegree )
hiddenY <- sapply(hiddenX, hiddenFunction) + rnorm(length(hiddenX), mean=1000, sd=10000)
```


```{r}
# Iterate through and perform regression with differen polynomial degrees.
hiddenDegreesToExplore <- 10
hiddenAICResults <- vector(mode = "numeric", length=hiddenDegreesToExplore)
hiddenBICResults <- vector(mode = "numeric", length=hiddenDegreesToExplore)
for (degree in 1:hiddenDegreesToExplore) {
  regressionModel <- lm(hiddenY ~ poly(hiddenX, degree = degree))
  hiddenAICResults[degree] <- AIC(regressionModel)
  hiddenBICResults[degree] <- BIC(regressionModel)
}
```

Let's visualize the AIC/BIC against the degree of polynomials.

```{r}
# Store in a dataframe to plot out
hiddenDegreesDF <- data.frame(degrees = 1:hiddenDegreesToExplore, AICResults = AICResults, BICResults = BICResults)
hiddenDegreesP <- ggplot(hiddenDegreesDF, aes(x=degrees)) +
  geom_line(aes(y=AICResults, color="AICResults")) +
  geom_line(aes(y=BICResults, color="BICResults")) +
  scale_x_continuous(breaks = 1:hiddenDegreesToExplore) + 
  ggtitle("Polynomial Degree vs AIC & BIC") +
  labs(y="AIC/BIC", x = "Polynomial Degree") +
  geom_segment(aes(x=hiddenDegree, y=24000, xend=hiddenDegree, yend=22500),
               arrow = arrow(length = unit(0.3, "cm"),
                             angle=20, type="closed"))
hiddenDegreesP
```

```{r}
hiddenDegree
```

The data was generated with 8 polynomial degrees which corresponds to the AIC/BIC graph above. 1-7 degrees was not good enough to represent the data. 8-10 though was good enough. Possibly, the difference between 8-10 degrees were miniscule enough to not generate an overfitting.

## Logistic Regression
We will be doing logistic regression based on http://michael.hahsler.net/SMU/EMIS7332/R/logistic_regression.html

The data we will be using is the [Iris flower data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) contains data about various flower features such as its sepal length, sepal width, petal length, and petal width. For every entry, each has those features along with the labelled species.

```{r}
iris
```

```{r message=FALSE, warning=FALSE}
ggpairs(iris, aes(colour = Species, alpha = 0.4), columns = 1:4, title = "Iris Pairwise Features plot")
```


### Some preprocessing

We will have to do some preprocessing to filter out the dataset, convert labels into 0 and 1, and generate train (the set where we do logistic regression on) and test (set to measure performance). We will be filtering out for the classes setosa and versicolor.

```{r}
# Filter for desired classes
onlySetosa = iris[iris$Species == "setosa", ]
onlyVersicolor = iris[iris$Species == "versicolor", ]
# Create train and test 75/25 split
# Surprise, another seed
set.seed(410)
setosaSplit <- sample.int(n = nrow(onlySetosa), size = floor(0.75 * nrow(onlySetosa)), replace = F)
versicolorSplit <- sample.int(n = nrow(onlyVersicolor), size = floor(0.75 * nrow(onlyVersicolor)), replace = F)
trainSetosa <- onlySetosa[setosaSplit,]
trainVersicolor <- onlyVersicolor[versicolorSplit, ]
testSetosa <- onlySetosa[-setosaSplit, ]
testVersicolor <- onlyVersicolor[-versicolorSplit, ]
irisTrain <- rbind(trainSetosa, trainVersicolor)
irisTest <- rbind(testSetosa, testVersicolor)
# Fix up the metadata since we're skipping a class
irisTrain$Species <- factor(irisTrain$Species)
irisTest$Species <- factor(irisTest$Species)
irisTrain
irisTest
```

### Pre Logistic regression

Let's try plotting the data on a 2d plot between all the combinations of features.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
filteredDataset <- rbind(onlySetosa, onlyVersicolor)
ggpairs(filteredDataset, aes(colour = Species, alpha = 0.4), columns = 1:4, title = "Pairwise Features plot")
```

The dataset is nicely segregated. Just choosing any pairwise feature for logistic regression is suffice. Might as well do logistic regression on all the features then.

### The actual logistic regression

Performing logistic regression in R is pretty simple. Just use the `glm()` function.

```{r warning=FALSE}
irisRegression <- glm(formula=Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, family=binomial, data=irisTrain)
```

Let's see how well it did on train and test groups.

```{r}
# Train
irisTrainPrediciton <- predict(irisRegression, irisTrain, type="response")
table(actual=irisTrain$Species, predicted=irisTrainPrediciton>.5)
```

```{r}
# Test
irisTestPrediction <- predict(irisRegression, irisTest, type="response")
table(actual=irisTest$Species, predicted=irisTestPrediction>.5)
```

Well that was an easy 100%. Judging from the pairwise plot for setosa and versicolor, I guess the results do make sense since it's so segregated. Now let's try versicolor and virginica.


```{r}
onlyVirginica = iris[iris$Species == "virginica", ]
# Same deal with train and test
set.seed(410)
versicolorSplit <- sample.int(n = nrow(onlyVersicolor), size = floor(0.75 * nrow(onlyVersicolor)), replace = F)
virginicaSplit <- sample.int(n = nrow(onlyVirginica), size = floor(0.75 * nrow(onlyVirginica)), replace = F)
trainVersicolor <- onlyVersicolor[versicolorSplit, ]
trainVirginica <- onlyVirginica[virginicaSplit,]
testVersicolor <- onlyVersicolor[-versicolorSplit, ]
testVirginica <- onlyVirginica[-virginicaSplit, ]
irisTrain <- rbind(trainVersicolor, trainVirginica)
irisTest <- rbind(testVersicolor, testVirginica)
# Fix up the metadata since we're skipping a class
irisTrain$Species <- factor(irisTrain$Species)
irisTest$Species <- factor(irisTest$Species)
irisTrain
irisTest
```

Do another pairwise plot

```{r}
filteredDataset <- rbind(onlyVersicolor, onlyVirginica)
ggpairs(filteredDataset, aes(colour = Species, alpha = 0.4), columns = 1:4, title = "Pairwise Features plot")
```

Hmm, still nicely segregated.

```{r}
irisRegression <- glm(formula=Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, family=binomial, data=irisTrain)
```

```{r}
# Train
irisTrainPrediciton <- predict(irisRegression, irisTrain, type="response")
table(actual=irisTrain$Species, predicted=irisTrainPrediciton>.5)
```


```{r}
# Test
irisTestPrediction <- predict(irisRegression, irisTest, type="response")
table(actual=irisTest$Species, predicted=irisTestPrediction>.5)
```

Not 100% on the training set but 100% on the test set. What if we just use sepal features?

```{r}
irisRegression <- glm(formula=Species ~ Sepal.Length + Sepal.Width, family=binomial, data=irisTrain)
```

```{r}
# Train
irisTrainPrediciton <- predict(irisRegression, irisTrain, type="response")
trainConfusion <- table(actual=irisTrain$Species, predicted=irisTrainPrediciton>.5)
trainConfusion
```

```{r}
# Test
irisTestPrediction <- predict(irisRegression, irisTest, type="response")
testConfusion <- table(actual=irisTest$Species, predicted=irisTestPrediction>.5)
testConfusion
```

Not 100% this time. Let's calculate the f1 scores

```{r}
f1Score <- function(confusionMatrix) {
  return((2 * confusionMatrix[4]) / ((2 * confusionMatrix[4]) + confusionMatrix[2] + confusionMatrix[3]))
}
f1Score(trainConfusion)
f1Score(testConfusion)
```

## Conclusion

We went through various metrics to determine how good a model is.

For linear and polynomial regresison we used cost function analysis and AIC and BIC analysis. For logistic we used f1 scores in this exercise. There are more types of analysis that can be explored that is not part of this exercise. What you choose ultimately depends on what the model what type of regression was used.
