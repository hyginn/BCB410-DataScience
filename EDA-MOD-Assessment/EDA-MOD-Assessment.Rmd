---
title: "EDA-MOD-Assessment"
author: Adriel Martinez (adriel.martinez@mail.utoronto.ca)
output: html_document
---

NOTE: If you want to go through this notebook, clear all the code output. Click the gear icon beside preview and click _Clear All Output_


## How to use Rmarkdown
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
plot(cars)
```

## Some prep

We will be using a couple of libraries.

```{r message=FALSE, warning=FALSE}
if (!require(ggplot2, quietly=TRUE)) {
  install.packages("ggplot2")
  library(ggplot2)
}

if (!require(GGally, quietly=TRUE)) {
  install.packages("GGally")
  library(GGally)
}
```

Some cost functions will be used

```{r}
sumOfSquaresCost <- function(x, y, theta) {
  X <- matrix(c(1), ncol=length(x))
  X <- rbind(X, x)
  return(sum((y - (t(theta) %*% X)) ^ 2))
}

sumOfSquaresGrad <- function(x, y, theta) {
  X <- matrix(c(1), ncol=length(x))
  X <- rbind(X, x)
  tmp <- y - (t(theta) %*% X)
  rowWise <- sweep(X,MARGIN=2, tmp,`*`)
  return(-2 * matrix(rowSums(rowWise)))
}

sigmoidFunction <- function(x) {
  return(1/(1 + exp(-x)))
}

logisticCost <- function(x, y, theta) {
  X <- matrix(c(1), ncol=length(x))
  X <- rbind(X, x)
  h <- sapply(t(theta) %*% X, sigmoidFunction)
  totalCost <- 0
  for (i in 1:length(h)) {
    elementCost <- -(y[i] * log(h[i])) - ((1 - y) * log(1 - h[i]))
    totalCost <- totalCost + elementCost
  }
  return(totalCost)
}

logisticCostGrad <- function(x, y, theta) {
  X <- matrix(c(1), ncol=length(x))
  X <- rbind(X, x)
  h <- sapply(t(theta) %*% X, sigmoidFunction)
  tmp <- h - y
  rowWise <- sweep(X,MARGIN=2, tmp,`*`)
  return(matrix(rowSums(rowWise)))
}

polySumOfSquaresCost <- function(x, y, genFunction) {
  return(sum((y - sapply(x, genFunction)) ^ 2))
}
```

I guess gradient descent will be used as well.

```{r}
gradDescent <- function(x, y, theta, costFun, costGrad, alpha, maxIter, verbose=FALSE) {
  cost <- c(length=maxIter + 1)
  cost[1] <- costFun(x, y, theta)
  iter <- 0
  while (iter < maxIter) {
    theta <- theta - (alpha * costGrad(x, y, theta))
    iterCost <- costFun(x, y, theta)
    cost[iter + 2] <- iterCost
    if (verbose) {
      print(sprintf("Iter: %f/%f, Cost: %f", iter + 1, maxIter, iterCost))
    }
    iter <- iter + 1
  }
  return(list("theta"=theta, "cost"=cost))
}
```

## Linear Data Regression

### Generate some data

We will be generating data around the line y = 22x - 35

```{r}
linearTheta <- matrix(c(-35, 22))
dataSize <- 2000
# Don't forget that seed
set.seed(123)
# Generate the x values between -20 and 20
linearX <- runif(dataSize, min=-20, max=20)
# To generate y, we need to append a row of 1's for the intercept
linearM <- matrix(c(1), ncol=length(linearX))
linearM <- rbind(linearM, linearX)
# The actual generation. Do matrix multiplication to generate the line values then add normal noise
linearY <- c(t(linearTheta) %*% linearM + rnorm(length(linearX), mean=10, sd=100))
```

Let's see the generated data.

```{r}
# Store in a data frame so we can plot it
linearDf <- data.frame(x=linearX, y=linearY)
linearPlot <- ggplot(linearDf, aes(x, y)) +
  geom_point(colour="dodgerblue",alpha=0.75) +
  geom_abline(aes(colour="green", intercept=linearTheta[1], slope=linearTheta[2]), alpha=0.8, size=1) +
  scale_color_identity(labels=c("True Line"), guide="legend") +
  ggtitle("Synthetic Data (Linear Data)") +
  labs(y="The Y value", x = "The X feature")
linearPlot
```

### The actual Linear Regression

Fitting a regression line is as _easy_ as running `lm()`.

```{r}
linearDataRegression <- lm(linearY ~ linearX)
linearDataRegression
```

Pretty close to the true line.

```{r message=FALSE, warning=FALSE}
linearPlot <- linearPlot +
  geom_abline(aes(colour="black", intercept=linearDataRegression$coefficients[1], slope=linearDataRegression$coefficients[2]), alpha=0.8, size=1) +
  scale_color_identity(labels=c("Regression Line", "True Line"), guide="legend")
linearPlot
```

Let's try using gradient descent. The code is available in the __Some Prep__ section.

```{r}
initTheta <- matrix(c(1, 2))
gradResults <- gradDescent(linearX, linearY, initTheta, sumOfSquaresCost, sumOfSquaresGrad, 3E-9, 5000)
gradResults$theta
```

Again pretty close. Plot it as well.

```{r message=FALSE, warning=FALSE}
linearPlot <- linearPlot +
  geom_abline(aes(colour="red", intercept=gradResults$theta[1], slope=gradResults$theta[2]), alpha=0.8, size=1) +
  scale_color_identity(labels=c("Regression Line", "Real Line", "Gradient Descent"), guide="legend")
linearPlot
```

Let's see those learning (cost) curves.

```{r}
gradDf <- data.frame(iter=1:length(gradResults$cost), cost=gradResults$cost)
costP <- ggplot(gradDf, aes(iter, cost)) +
  geom_point(colour="red",alpha=0.75, shape=42) +
  ggtitle("Gradient Descent: Cost vs Iterations") +
  labs(y="Sum of Squares", x = "Iterations")
costP
```
 
That's a smooth descent!

### Some metrics analysis

Let's look at the sum of squares error and mean squared error.

```{r}
# Real line
sumOfSquaresCost(linearX, linearY, linearTheta)
sumOfSquaresCost(linearX, linearY, linearTheta) / length(linearX)
# Built in R lm (Maximum likelihood estimator?)
mleTheta <- matrix(c(linearDataRegression$coefficients))
sumOfSquaresCost(linearX, linearY, mleTheta)
sumOfSquaresCost(linearX, linearY, mleTheta) / length(linearX)
# Gradient Descent
sumOfSquaresCost(linearX, linearY, gradResults$theta)
sumOfSquaresCost(linearX, linearY, gradResults$theta) / length(linearX)
```

The R lm algorithm did better than the real line. That's the power of randomness!

R includes some analysis in the regression model.

```{r}
summary(linearDataRegression)
```

## Nonlinear Data Regression

### Generate some data

This time, we'll generate data points for y = 2x^3^ - x^2^ + 3x - 33

```{r}
# First off, let's create a function that will create a function for a particular theta
genYFunction <- function(t) {
  genY <- function(x) {
    return((t[4] * (x ^ 3)) + (t[3] * (x ^ 2)) + (t[2] *  x) - t[1])
  }
  return(genY)
}

polyTheta <- matrix(c(-33, 3, 1, 2))
dataSize <- 1000
# Another. 10 because class starts at 10.
set.seed(10)
polyX <- runif(dataSize, min=-20, max=20)
realFunction <- genYFunction(polyTheta)
polyY <- sapply(polyX, realFunction) + rnorm(length(polyX), mean=32, sd=3000)
```

It should look fine? Let's plot it.

```{r}
# Store in df to plot
polyDf <- data.frame(x=polyX, y=polyY)
polyP <- ggplot(polyDf, aes(x, y)) +
  geom_point(colour="dodgerblue",alpha=0.75) +
  stat_function(fun = realFunction, aes(colour = "black")) +
  scale_color_identity(labels=c("True Line"), guide="legend") +
  ggtitle("Synthetic Data (Polynomial)") +
  labs(y="The Y value", x = "The X feature")
polyP
```

### Regression

Now we regress. We still use `lm()`

```{r}
# While we still use lm(), the parameters are different now
nonlinReg <- lm(polyY ~ polyX + I(polyX^2) + I(polyX^3))
nonlinReg
```

```{r message=FALSE, warning=FALSE}
polyFitFunction <- genYFunction(nonlinReg$coefficients)
polyP <- polyP +
  stat_function(fun = polyFitFunction, aes(colour = "red")) +
  scale_color_identity(labels=c("True Line", "Poly Regression Line"), guide="legend")
polyP
```

Pretty close? What if we thought the data was linear?

```{r}
linReg <- lm(polyY ~ polyX)
linReg
```

```{r message=FALSE, warning=FALSE}
polyP <- polyP +
  geom_abline(aes(colour="green", intercept=linReg$coefficients[1], slope=linReg$coefficients[2]), alpha=0.8, size=1) +
  scale_color_identity(labels=c("True Line", "Lin Regression Line", "Poly Regression Line"), guide="legend")
polyP
```

Well that doesn't look right.

### Some metrics analysis

Let's look at sum of squares and mean squared error.

```{r}
# Real line
polySumOfSquaresCost(polyX, polyY, realFunction)
polySumOfSquaresCost(polyX, polyY, realFunction) / length(polyX)
# Polynomial regression
polySumOfSquaresCost(polyX, polyY, polyFitFunction)
polySumOfSquaresCost(polyX, polyY, polyFitFunction) / length(polyX)
# Linear regression
mleTheta <- matrix(c(linReg$coefficients))
sumOfSquaresCost(polyX, polyY, mleTheta)
sumOfSquaresCost(polyX, polyY, mleTheta) / length(polyX)
```

This time, the estimated polynomial line didn't do as good as the real line. The linear line was not even close...

```{r}
summary(nonlinReg)
summary(linReg)
```


## Logistic Regression
We will be doing logistic regression based on http://michael.hahsler.net/SMU/EMIS7332/R/logistic_regression.html

The data we will be using is the [Iris flower data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) contains data about various flower features such as its sepal length, sepal width, petal length, and petal width. For every entry, each has those features along with the labelled species.

```{r}
iris
```

```{r message=FALSE, warning=FALSE}
ggpairs(iris, aes(colour = Species, alpha = 0.4), columns = 1:4, title = "Iris Pairwise Features plot")
```


### Some preprocessing

We will have to do some preprocessing to filter out the dataset, convert labels into 0 and 1, and generate train (the set where we do logistic regression on) and test (set to measure performance). We will be filtering out for the classes setosa and versicolor.

```{r}
# Filter for desired classes
onlySetosa = iris[iris$Species == "setosa", ]
onlyVersicolor = iris[iris$Species == "versicolor", ]
# Create train and test 75/25 split
# Surprise, another seed
set.seed(410)
setosaSplit <- sample.int(n = nrow(onlySetosa), size = floor(0.75 * nrow(onlySetosa)), replace = F)
versicolorSplit <- sample.int(n = nrow(onlyVersicolor), size = floor(0.75 * nrow(onlyVersicolor)), replace = F)
trainSetosa <- onlySetosa[setosaSplit,]
trainVersicolor <- onlyVersicolor[versicolorSplit, ]
testSetosa <- onlySetosa[-setosaSplit, ]
testVersicolor <- onlyVersicolor[-versicolorSplit, ]
irisTrain <- rbind(trainSetosa, trainVersicolor)
irisTest <- rbind(testSetosa, testVersicolor)
# Fix up the metadata since we're skipping a class
irisTrain$Species <- factor(irisTrain$Species)
irisTest$Species <- factor(irisTest$Species)
irisTrain
irisTest
```

### Pre Logistic regression

Let's try plotting the data on a 2d plot between all the combinations of features.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
filteredDataset <- rbind(onlySetosa, onlyVersicolor)
ggpairs(filteredDataset, aes(colour = Species, alpha = 0.4), columns = 1:4, title = "Pairwise Features plot")
```

The dataset is nicely segregated. Just choosing any pairwise feature for logistic regression is suffice. Might as well do logistic regression on all the features then.

### The actual logistic regression

Performing logistic regression in R is pretty simple. Just use the `glm()` function.

```{r warning=FALSE}
irisRegression <- glm(formula=Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, family=binomial, data=irisTrain)
```

Let's see how well it did on train and test groups.

```{r}
# Train
irisTrainPrediciton <- predict(irisRegression, irisTrain, type="response")
table(actual=irisTrain$Species, predicted=irisTrainPrediciton>.5)
```

```{r}
# Test
irisTestPrediction <- predict(irisRegression, irisTest, type="response")
table(actual=irisTest$Species, predicted=irisTestPrediction>.5)
```

Well that was an easy 100%. Judging from the pairwise plot for setosa and versicolor, I guess the results do make sense since it's so segregated. Now let's try versicolor and virginica.


```{r}
onlyVirginica = iris[iris$Species == "virginica", ]
# Same deal with train and test
set.seed(410)
versicolorSplit <- sample.int(n = nrow(onlyVersicolor), size = floor(0.75 * nrow(onlyVersicolor)), replace = F)
virginicaSplit <- sample.int(n = nrow(onlyVirginica), size = floor(0.75 * nrow(onlyVirginica)), replace = F)
trainVersicolor <- onlyVersicolor[versicolorSplit, ]
trainVirginica <- onlyVirginica[virginicaSplit,]
testVersicolor <- onlyVersicolor[-versicolorSplit, ]
testVirginica <- onlyVirginica[-virginicaSplit, ]
irisTrain <- rbind(trainVersicolor, trainVirginica)
irisTest <- rbind(testVersicolor, testVirginica)
# Fix up the metadata since we're skipping a class
irisTrain$Species <- factor(irisTrain$Species)
irisTest$Species <- factor(irisTest$Species)
irisTrain
irisTest
```

Do another pairwise plot

```{r}
filteredDataset <- rbind(onlyVersicolor, onlyVirginica)
ggpairs(filteredDataset, aes(colour = Species, alpha = 0.4), columns = 1:4, title = "Pairwise Features plot")
```

Hmm, still nicely segregated.

```{r}
irisRegression <- glm(formula=Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, family=binomial, data=irisTrain)
```

```{r}
# Train
irisTrainPrediciton <- predict(irisRegression, irisTrain, type="response")
table(actual=irisTrain$Species, predicted=irisTrainPrediciton>.5)
```


```{r}
# Test
irisTestPrediction <- predict(irisRegression, irisTest, type="response")
table(actual=irisTest$Species, predicted=irisTestPrediction>.5)
```

Not 100% on the training set but 100% on the test set. What if we just use sepal features?

```{r}
irisRegression <- glm(formula=Species ~ Sepal.Length + Sepal.Width, family=binomial, data=irisTrain)
```

```{r}
# Train
irisTrainPrediciton <- predict(irisRegression, irisTrain, type="response")
trainConfusion <- table(actual=irisTrain$Species, predicted=irisTrainPrediciton>.5)
trainConfusion
```

```{r}
# Test
irisTestPrediction <- predict(irisRegression, irisTest, type="response")
testConfusion <- table(actual=irisTest$Species, predicted=irisTestPrediction>.5)
testConfusion
```

Not 100% this time. Let's calculate the f1 scores

```{r}
f1Score <- function(confusionMatrix) {
  return((2 * confusionMatrix[4]) / ((2 * confusionMatrix[4]) + confusionMatrix[2] + confusionMatrix[3]))
}
f1Score(trainConfusion)
f1Score(testConfusion)
```
