---
title: "APB-ML-Measuring_performance"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!-- ## R Markdown -->

<!-- This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. -->

<!-- When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: -->

<!-- ```{r cars}
summary(cars)
``` -->

<!-- ## Including Plots -->

<!-- You can also embed plots, for example: -->

<!-- ```{r pressure, echo=FALSE} -->
<!-- plot(pressure) -->
<!-- ``` -->

<!-- Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot. -->


# How to use this guide

Use this as a documentation for how to interpret the results from `APB-ML-Measuring_performance.R`.

To evaluate machine learning algorithms, you really only need 3 things:

1. The dataset (obviously)
2. Test options
3. Evaluation metrics

I will explain these three things in detail below.

## Requirements
```
install.packages(c("mlbench", "caret", "pROC", "klaR", "MLmetrics"), dependencies=TRUE)
```

## Test options
- **Split data** into training, test, and validation sets. Use your training set to train the model, use your validation set to fine tune the hyper-parameters, and then measure your model's predictive performance on the test set.

- **K-fold cross validation**: partition data into k subsets, alliteratively leave one subset out as test set, use the rest for training

- **Bootstrap resampling**: randomly sample from the dataset (with re-selection) against which to evaluate the model. In aggregate, the results provide an indication of the variance of the model's performance. Typically, a large number of re-sampling iterations are performed (thousands or tens of thousands).

*If you are unsure which test options to use, I recommend start by using a 10-fold cross validation with multiple runs.*


## How to choose a test option

### Understand randomness
The key concept to understand is randomness, a.k.a **stochasticity**. Most ML algorithms use some randomness in one way or another. Either the algorithm itself is random, such as stochastic gradient descent, or there is randomness within the training dataset. This does not mean that the predictions are going to be random, it implies that is **variance** in the predictions. Choosing the right test option is all about accounting for this variance.

### Why not just train and test on the same dataset?
Lets say you trained a model on a dataset, then measured its accuracy on the same dataset. You get an accuracy of 95%, that's the true accuracy of the algorithm on the dataset, right? Not exactly...

What you did is over-fit on the dataset for training, and have no idea how well your model can generalize on **new** data. 

Since we actually want to predict on new data, this is not a viable approach.

### Split data
*Advantages:*

- easy to implement

*Disadvantages:*

- you lose a good amount of data from training the model, so there's high bias to the training set

You take the dataset, and split it into a training and a test set (and maybe a third validation set for hyper-parameter search, but we won't cover that here).

<!-- Give an example with R code. -->
For example, you randomly shuffle the entire dataset, then use 2/3 as the training set and 1/3 as the test set. The algorithm runs on the training set, then the trained model is evaluated on the test set, and you get something like 85% accuracy.

```{r, echo=TRUE, message=FALSE}
# load the libraries
library(caret)
library(klaR)
# load the iris dataset
data(iris)
# define an 80%/20% train/test split of the dataset
split=0.80
trainIndex <- createDataPartition(iris$Species, p=split, list=FALSE)
data_train <- iris[ trainIndex,]
data_test <- iris[-trainIndex,]
# train a naive bayes model
model <- NaiveBayes(Species~., data=data_train)
# make predictions
x_test <- data_test[,1:4]
y_test <- data_test[,5]
predictions <- predict(model, x_test)
# summarize results
confusionMatrix(predictions$class, y_test)
```

This is a fast and easy approach when you have a lot of data. When the dataset is very big, splitting the data can actually yield a fairly accurate measure of the actual performance of the algorithm.

But how good is the algorithm really? How **confident** are we that its true positive rate is 87%?

Lets say we use a different random seed this time, and split the data the same way -- into 2/3 train and 1/3 test -- we would in fact get a different result.

#### How do we account for this variance?
We can try to reduce the variance of the random process by doing it multiple times and getting the average. For example, we do the split data above 10 times, each time we set a new seed and get a new accuracy, then we compute the average and standard deviations of those accuracies.

But there is still the possibility that some samples are never used as either training or testing, while some samples are used multiple times. This might cause the algorithm to over-fit on those samples scene multiple times.

**Solution**: *k-fold cross validation*.

### Cross validation (CV)

*Advantages:*

- CV uses more test data
- CV uses the entire training dataset for both training and evaluation, instead of some portion. In contrast, if you validate a model by using data generated from a random split, typically you evaluate the model only on 30% or less of the available data.
- CV evaluates the dataset as well as the model.
- CV does not simply measure the accuracy of a model, but also gives you some idea of how representative the dataset is and how sensitive the model might be to variations in the data.

*Disadvantages:*

- Because CV trains and validates the model multiple times over a larger dataset, it is much more computationally intensive and takes much longer than validating on a random split.
- If the dataset is very big, you may have to resort to either sampling the data or stick to a split test.
    
Here, CV specifically refers to k-fold cross validation, where k is the number of splits we make in the data.

<!-- Give an example with R code. -->
For example, suppose k=10. This will split the dataset into 10 subsets (folds), and the algorithm will iterate 10 times, each time it trains on 9 folds and tests on 1 without replacement. This way, each data sample will be used as training 9 times, and as test 1 time. The combined accuracy is not an average, but instead an exact accuracy measure on how many correct predictions were made.

Although CV does provide an unbiased estimate on the model's accuracy, it does not know a stochastic algorithm's internal randomness. This means it cannot account for the variance in the model's predictions. Furthermore, the splitting of data into folds is itself a random process, which means a single run does not estimate how the algorithm performs on different splits.

#### How do we account for the variance in the algorithm itself?
Run CV multiple times and take the average and standard deviation of the accuracies. The average gives us an estimate of the algorithm's *performance*, and the standard deviation helps us understand the *robustness* of the algorithm on the dataset.

For example,
```{r, echo=TRUE, message=FALSE}
# load the library
library(caret)
# load the iris dataset
data(iris)
# define training control
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
# summarize results
print(model)
```

### Bootstrap resampling
This approach involves randomly sampling from the dataset (with replacement) against which to evaluate the model.

For example, randomly sample the dataset 10 times
```{r, echo=TRUE, message=FALSE}
# load the library
library(caret)
# load the iris dataset
data(iris)
# define training control
train_control <- trainControl(method="boot", number=100)
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
# summarize results
print(model)
```

### How do we compare different algorithms?

Lets say after doing k-fold CV with multiple runs, algorithm A yields one set of mean and standard deviation and algorithm B yields a different set of mean and standard deviation, and A has a higher accuracy than B, how do you know if the difference is meaningful?

#### Statistical significance
We can use a statistical significance test (like the U test) to compare different algorithms.

If we run k-fold CV with multiple runs using different ML algorithms, then each algorithm produces a list of numbers. We summarize these numbers with a statistic (such as mean and standard deviation). A statistical significance test quantifies how likely these different sets of numbers are drawn from the same population. If this quantity (p-value) is above some threshold, then the difference in the statistics between each algorithm are insignificant.

<!-- When in doubt, use k-fold cross validation (k=10) and use multiple runs of k-fold cross validation with statistical significance tests when you want to meaningfully compare algorithms on your dataset. -->


## Error metrics

Alright! Suppose you trained a model using k-fold cross validation with multiple runs, and you believe it can make good predictions because you got an accuracy of 90%. How do you decide whether it is a **good enough** model to **solve your problem**. Classification accuracy alone is typically not enough information to make this decision. We need better metrics that can explain the performance of a model, more importantly, discriminate among different models.

Error metrics for classfication and regression models are different because they produce nominal or binary outputs and continuous outputs respectively.

#### Note: The Difference Between Training Metrics and Evaluation Metrics
Sometimes the metric being used by the loss function during training is different from the evaluation metric. This often happens when we are trying to use our model for a different task than it was designed for. For example, we might train a logistic regression model on for a genetic test by minimizing its Log loss, and then directly use it to predict on patients. However, what's the model's ability to identify true positives vs false negatives? How much do we care about false negatives? There should be a way to answer those questions.

The better way is to train the model using the metric that suits for your problem. However, some metrics this is hard to do. For example, it's hard to directly optimize for the AUC. Always think about the right evaluation metric, and see if it's possible to use it directly to train your model. 

### Classification metrics

#### Accuracy and Kappa
**Accuracy** = # correct predictions / total # of predictions

**Kappa** (Cohen's Kappa) is essentially a measure of how well the classifier performed as compared to how well it would have performed simply by chance. This is a helpful measure for datasets that have imbalanced classes.

#### Confusion Matrix (not a metric)
A confusion matrix is an N X N matrix, where N is the number of classes being predicted. For a binary classifier, the matrix would be 2 x 2. Although this matrix is not a metric, it helps us organize and see the important results from our predictions.

Example: First lets generate some data and evaluate a simple "binary classifier". We generate a dataset with skewed classes, where probability of Class1 is 10% and Class2 is 90%.
```{r, echo=TRUE, message=FALSE}
# Randomly sample from a PMF where Pr(Class1) = 0.2 and Pr(Class2) = 0.8
true_class <- factor(sample(
  paste0("Class", 1:2),
  size = 3000,
  prob = c(.1, .9),
  replace = TRUE
))
true_class <- sort(true_class)
# Sample from a beta distribution for the probabilities produced by a "model"
class1_probs <- rbeta(sum(true_class == "Class1"), 4, 1)
class2_probs <- rbeta(sum(true_class == "Class2"), 1, 2.5)
test_set <- data.frame(obs = true_class,
                       Class1 = c(class1_probs, class2_probs))
test_set$Class2 <- 1 - test_set$Class1
test_set$pred <-
  factor(ifelse(test_set$Class1 >= .5, "Class1", "Class2"))

# Plot the distribution of Class 1 probabilities
ggplot(test_set, aes(x = Class1)) +
  geom_histogram(binwidth = .05) +
  facet_wrap(~ obs) +
  xlab("Probability of Class #1")
```

```{r}
confusionMatrix(data = test_set$pred, reference = test_set$obs)
```

Noice that the accuracy is ~84% whereas the No Information Rate is 89%, which is basically the same as the 90% of our majority class (class2). The Kappa score is the normalized version, and reports a low accuracy of 48%.
There are a lot of terms and statistics presented here, so lets define some of them...

Definition of key terms:

- True positive (TP): predicted `a` and actual `a`
- True negative (TN): predicted `b` and actual `b`
- False positive (FP): predicted `a` but actual `b`
- False negative (FN): predicted `b` but actual `a`

Confusion matrix structure for binary classes:
```
            TRUTH
           POS  NEG
     POS | TP | FP |
PRED     -----------
     NEG | FN | TN |
```
- *no-information rate*: the largest proportion of the observed classes (in the example class 2 occurs much more than class 1)
- *P-Value [Acc > NIR]*: a hypothesis test is also computed to evaluate whether the overall accuracy rate is greater than the rate of the largest class
- *POS*: positive class
- *NEG*: negative class
- *Accuracy*: the proportion of the total number of predictions that were correct.
- *Kappa*: normalized accuracy.
- *Sensitivity* *(aka Recall)*: the proportion of actual positive cases which are correctly identified.
    - `recall = TP / (TP + FN)`
- *Specificity*: the proportion of actual NEG's which are correctly identified.
- *Pos Pred Value (aka Precision)*: the proportion of POS's that were correctly identified.
    - `precision = TP / (TP + FP)`
- *Neg Pred Value*: the proportion of NEG's that were correctly identified.
- *Prevalence*: how often does the POS condition actually occur in our sample?
- *Detection Rate*: How likely is it to detect a true positive?
- *Detection Prevalence*: Overall, how often is it wrong?

Lets explore precision and recall in more detail below.

#### Precision/Recall and F1 Score

We can get precision recall metrics from the `confusionMatrix()` method as well.
```{r}
confusionMatrix(data = test_set$pred, reference = test_set$obs, mode = "prec_recall")
```

Precision and recall are actually two different metrics. But they are often used together. **Precision** answers the question, "Out of the samples that the model predicted to be positive, how many are truly positive?" The **recall** answers the question, "Out of all the samples that are truly positive, how many can be detected by the model?"

![Illustration from Wikipedia:](https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg)

Often you can specify a maximum length `k` for the predictions. This is call called "precision at k" and "recall at k". By varying the value of `k`, the precision and recall vary as well. We can plot the precision vs recall at varying levels of `k`, very much like an ROC curve (explained below). However, we would like to use a single score to reprsent how good a model is based on its precision/recall, and this is the F1 Score.

The F1 Score is a weighted average of the true positive rate (recall) and precision. It is also defined as the harmonic mean between precision and recall:
 
F1 = 2( (precision * recall) / (precision + recall))

Therefore, the F1 score will be small if either precision ore recall is small.

```{r}
# precision-recall values and the area under the precision-recall curve
prSummary(test_set, lev = levels(test_set$obs))
```
Here, the AUC is actually the area under the Precision vs Recall curve, and not the ROC.

#### AUC (Area Under ROC Curve)

The AUC is an error metric for binary classifiers, while the ROC is a plot that visualizes the performance of a classifier over all possible thresholds. The true positive rate (y-axis) is plotted against the false positive rate (x-axis) as you vary the threshold for assigning observations to a given class.

The AUC is a measure of the model's ability to discriminate between two classes. An area of 1.0 represents a model predicted every test sample correctly. An area of 0.5 means the model is as a good as a fair coin.

```{r}
# Note that the ROC score should really be the AUC score
twoClassSummary(test_set, lev = levels(test_set$obs))
```

How to plot a ROC

```{r}
# load libraries
library(caret)
library(plotROC)

# Use ggplot2 and plotROC (an extension to ggplot2 for ROC)
ggplot(test_set, aes(d = obs, m = Class2)) + geom_roc(n.cuts = 5) + style_roc() + geom_rocci(sig.level = .01)

```

This plot shows us that:
- the true positive rate = ~0.8 at 0.5 threshold, and a corresponding ~0.07 false positive rate
- this means that the classifier is pretty good
- the numbers on the lines are the thresholds
- the size of the grey boxes are confidence regions for both the Y and X axis at selected thresholds, calculated by the Clopper and Pearson (1934) exact method.

#### Log-loss
Log-loss is more prevalent for multiclass classifiers. Log-loss evaluates the probabilities produced by classifiers. The probabilities can be viewed as a bayesian posterior probability of your model's predictions given the data observed. For example, if the true label is `1`, but the classifier thinks it should be `0` with a probability of `0.51`, then it is really not making such a big mistake because it is simply not that confident about its prediction.

Another relationship to log-loss is information. The log-loss is the cross entropy between the destribution of the true labels and the predictions. By minimizing the cross entropy, we maximize the accuracy of the classifier.

```{r}
# compute the Log Loss on the test set
mnLogLoss(test_set, lev = levels(test_set$obs))
```

### Regression metrics

Since the output of regression models is continuous, the go to metrics are different than classifiers. The go to metrics are RMSE and R-squared. Let's go over both!

#### Root Mean Squared Error (RMSE)
RMSE assumes that the error are unbiased and follow a normal distribution. RMSE is the average deviation from of the predictions from the truths.

$$
RMSE = \sqrt\frac{\sum_{k=1}^n(Predicted_i - Actual_i)^2}{N}
$$

Key things to remember:

- The power of ‘square root’  empowers this metric to show large number deviations.
- The ‘squared’ nature of this metric helps to deliver more robust results which prevents cancelling the positive and negative error values. In other words, this metric aptly displays the plausible magnitude of error term.
- It avoids the use of absolute error values which is highly undesirable in mathematical calculations.
When we have more samples, reconstructing the error distribution using RMSE is considered to be more reliable.
- RMSE is highly affected by outlier values. Hence, make sure you’ve removed outliers from your data set prior to using this metric.
- As compared to mean absolute error, RMSE gives higher weightage and punishes large errors.

#### R-squared
R-squared provides a “goodness of fit” measure for the predictions to the observations. This is a value between 0 and 1 for no-fit and perfect fit respectively.


Example: linear regression with 5-fold CV:
```{r}
# load libraries
library(caret)
# load data
data(longley)
# prepare resampling method
control <- trainControl(method="cv", number=5)
set.seed(7)
fit <- train(Employed~., data=longley, method="lm", metric="RMSE", trControl=control)
# display results
print(fit)
```

Notice that using `RMSE` metric gives us both RMSE and Rsquared results.


## Summary

- the metric you select to optimize your model is important
- the metric you use to evaluate your model is important
- use measure appropriate for **your problem**
- accuracy is often not sufficent/appropriate
- AUC/ROC is pretty popular, but only appropriate for binary classes
- only accuracy generalizes to >2 classes!