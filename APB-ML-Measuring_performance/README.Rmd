---
title: "APB-ML-Measuring_performance"
output:
  html_document: default
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

This is the documentation for how to interpret the results from `APB-ML-Measuring_performance/APB-ML-Measuring_performance.R`.

Our goal is to evaluate a model's performance. To realize this goal, we need 3 things:

1. The dataset (obviously)
2. Method for testing
3. Evaluation metrics

Lets explore them in detail below.

## Requirements
```
install.packages(c("mlbench", "caret", "pROC", "klaR", "MLmetrics"), dependencies=TRUE)
```

##
```{r}
library(caret)
library(ggplot2)
library(plotROC)
library(mlbench)
```

We will primarily be using the `caret` library for computing the metrics for both classification and regression. All the code in this unit is based on the `caret` library, and you can find more details in this [reference](http://topepo.github.io/caret/measuring-performance.html).

# The Dataset
Hopefully you have a clean, balanced, and post-processed dataset. However, it's not required for this unit, since we will use simple synthetic datasets as a proof of concept. However, to use what you learn from this unit on your own dataset, then please go over the units that cover data preprocessing.

# Method for testing
- **Split data** into training, test, and validation sets. Use your training set to train the model, use your validation set to fine tune the hyper-parameters, and then measure your model's predictive performance on the test set.

- **K-fold cross validation**: partition data into k subsets, alliteratively leave one subset out as test set, use the rest for training

- **Bootstrap resampling**: randomly sample from the dataset (with re-selection) against which to evaluate the model. In aggregate, the results provide an indication of the variance of the model's performance. Typically, a large number of re-sampling iterations are performed (thousands or tens of thousands).

*If you are unsure which test options to use, I recommend start by using a 10-fold cross validation with multiple runs.*

## Which method to use?

### # Randomness
The data we work with is not perfect, that means it contains a certain amount of noise, which can be from some random process, a.k.a a **stochastic process**. Furthermore, most ML algorithms themselves use some randomness in one way or another. Hence, randomness is everywhere! However, this does not mean that the predictions are going to be random, rather it implies that there is **variance** in the predictions. Choosing the right method for testing is all about accounting for this variance.

#### Why not just train and test on the same dataset?
Lets say you trained a model on a dataset, then measured its accuracy on the same dataset. You get an accuracy of 95%, that's the true accuracy of the algorithm on the dataset, right? Not exactly...

What you did is over-fit on the dataset for training, and have no idea how well your model can generalize on **new** data. 

Since we actually want to predict on new data, this is not a viable approach.

### Split data
*Advantages:*

- easy to implement

*Disadvantages:*

- you lose a good amount of data from training the model, so there's high bias to the training set

You take the dataset, and split it into a training and a test set (and maybe a third validation set for hyper-parameter search, but we won't cover that here).

For example, you randomly shuffle the entire dataset, then use 2/3 as the training set and 1/3 as the test set. The algorithm runs on the training set, then the trained model is evaluated on the test set, and you get something like 85% accuracy.

This is a fast and easy approach when you have a lot of data. When the dataset is very big, splitting the data can actually yield a fairly accurate measure of the actual performance of the algorithm.

But how good is the algorithm really? How **confident** are we that its true positive rate is 87%?

Lets say we use a different random seed this time, and split the data the same way -- into 2/3 train and 1/3 test -- we would in fact get a different result.

#### How do we account for this variance?
We can try to reduce the variance of the random process by doing it multiple times and getting the average. For example, we do the split data above 10 times, each time we set a new seed and get a new accuracy, then we compute the average and standard deviations of those accuracies.

But there is still the possibility that some samples are never used as either training or testing, while some samples are used multiple times. This might cause the algorithm to over-fit on those samples scene multiple times.

**Solution**: *k-fold cross validation*.

### Cross validation (CV)

*Advantages:*

- CV uses more test data
- CV uses the entire training dataset for both training and evaluation, instead of some portion. In contrast, if you validate a model by using data generated from a random split, typically you evaluate the model only on 30% or less of the available data.
- CV evaluates the dataset as well as the model.
- CV does not simply measure the accuracy of a model, but also gives you some idea of how representative the dataset is and how sensitive the model might be to variations in the data.

*Disadvantages:*

- Because CV trains and validates the model multiple times over a larger dataset, it is much more computationally intensive and takes much longer than validating on a random split.
- If the dataset is very big, you may have to resort to either sampling the data or stick to a split test.
    
Here, CV specifically refers to k-fold cross validation, where k is the number of splits we make in the data.

<!-- Give an example with R code. -->
For example, suppose k=10. This will split the dataset into 10 subsets (folds), and the algorithm will iterate 10 times, each time it trains on 9 folds and tests on 1 without replacement. This way, each data sample will be used as training 9 times, and as test 1 time. The combined accuracy is not an average, but instead an exact accuracy measure on how many correct predictions were made.

Although CV does provide an unbiased estimate on the model's accuracy, it does not know a stochastic algorithm's internal randomness. This means it cannot account for the variance in the model's predictions. Furthermore, the splitting of data into folds is itself a random process, which means a single run does not estimate how the algorithm performs on different splits.

#### How do we account for the variance in the algorithm itself?
Run CV multiple times and take the average and standard deviation of the accuracies. The average gives us an estimate of the algorithm's *performance*, and the standard deviation helps us understand the *robustness* of the algorithm on the dataset.

### Bootstrap resampling
This approach involves randomly sampling from the dataset (with replacement) against which to evaluate the model.

### How do we compare different algorithms?

Lets say after doing k-fold CV with multiple runs, algorithm A yields one set of mean and standard deviation and algorithm B yields a different set of mean and standard deviation, and A has a higher accuracy than B, how do you know if the difference is meaningful?

#### Statistical significance
We can use a statistical significance test (like the U test) to compare different algorithms.

If we run k-fold CV with multiple runs using different ML algorithms, then each algorithm produces a list of numbers. We summarize these numbers with a statistic (such as mean and standard deviation). A statistical significance test quantifies how likely these different sets of numbers are drawn from the same population. If this quantity (p-value) is above some threshold, then the difference in the statistics between each algorithm are insignificant.

<!-- =========================================================================== -->

# Evaluation metrics (Error metircs)

Suppose you trained a model using k-fold cross validation with multiple runs, and you believe it can make good predictions because you got an accuracy of 90%. How do you decide whether it is a **good enough** model to **solve your problem**. Classification accuracy alone is typically not enough information to make this decision. We need better metrics that can explain the performance of a model, more importantly, discriminate among different models.

Error metrics for classfication and regression models are different because they produce nominal or binary outputs and continuous outputs respectively.

#### Note: The Difference Between Training Metrics and Evaluation Metrics
Sometimes the metric being used by the loss function during training is different from the evaluation metric. This often happens when we are trying to use our model for a different task than it was designed for. For example, we might train a logistic regression model on for a genetic test by minimizing its Log loss, and then directly use it to predict on patients. However, what's the model's ability to identify true positives vs false negatives? How much do we care about false negatives? There should be a way to answer those questions.

The better way is to train the model using the metric that suits your problem. However, some metrics is difficult to work with. For example, it's hard to directly optimize for the AUC. Always think about the right evaluation metric, and see if it's possible to use it directly to train your model. 

<!-- =========================================================================== -->

## Classification metrics

First lets generate some test data for this section, which we will use to evaluate a simple "binary classifier". We generate a dataset with skewed classes, with a 20,80-split

```{r}
ggplot(test_set, aes(x = Class1)) +
  geom_histogram(binwidth = .05) +
  facet_wrap(~ obs) +
  xlab("Distribution of Probabilities for Class #1")
```

[Reference: caret documentation](http://topepo.github.io/caret/measuring-performance.html)

### Accuracy and Kappa
**Accuracy** = # correct predictions / total # of predictions

**Kappa** (Cohen's Kappa) is essentially a measure of how well the classifier performed as compared to how well it would have performed simply by chance. This is a helpful measure for datasets that have imbalanced classes.

[Reference](https://en.wikipedia.org/wiki/Cohen%27s_kappa)

```{r}
postResample(pred = test_set$pred, obs = test_set$obs)
```

### Confusion Matrix (not a metric)
A confusion matrix is an N X N matrix, where N is the number of classes being predicted. For a binary classifier, the matrix would be 2 x 2. Although this matrix is not a metric, it helps us organize and see the important results from our predictions.

There are a lot of terms and statistics involved here, so lets define some of them...

Definition of key terms:

- True positive (TP): predicted `a` and actual `a`
- True negative (TN): predicted `b` and actual `b`
- False positive (FP): predicted `a` but actual `b`
- False negative (FN): predicted `b` but actual `a`

Confusion matrix structure for binary classes:
```
            TRUTH
           POS  NEG
     POS | TP | FP |
PRED     -----------
     NEG | FN | TN |
```
- *no-information rate*: the largest proportion of the observed classes (in the example class 2 occurs much more than class 1)
- *P-Value [Acc > NIR]*: a hypothesis test is also computed to evaluate whether the overall accuracy rate is greater than the rate of the largest class
- *POS*: positive class
- *NEG*: negative class
- *Accuracy*: the proportion of the total number of predictions that were correct.
- *Kappa*: normalized accuracy.
- *Sensitivity* *(aka Recall)*: the proportion of actual positive cases which are correctly identified.
    - `recall = TP / (TP + FN)`
- *Specificity*: the proportion of actual NEG's which are correctly identified.
- *Pos Pred Value (aka Precision)*: the proportion of POS's that were correctly identified.
    - `precision = TP / (TP + FP)`
- *Neg Pred Value*: the proportion of NEG's that were correctly identified.
- *Prevalence*: how often does the POS condition actually occur in our sample?
- *Detection Rate*: How likely is it to detect a true positive?
- *Detection Prevalence*: Overall, how often is it wrong?

```{r}
# Lets see what a confusion matrix looks like, as well as the accompanying statisics.
confusionMatrix(data = test_set$pred, reference = test_set$obs)
```

Noice that the accuracy is ~84% whereas the No Information Rate is 89%, which is basically the same as the 80% of our majority class (class2). The Kappa score reports a low accuracy of ~54%, which makes sense due to the class imbalance inherent in the dataset.


### Log-Loss
Although we use Log-Loss here for a binary classifier, it can generalize to multiple classes.  Log-Loss evaluates the probabilities produced by classifiers. The probabilities can be viewed as a bayesian posterior probability of your model's predictions given the data observed. For example, if the true label is `1`, but the classifier thinks it should be `0` because it reports a probability `0.48` for `1`, then it is really not making such a big mistake because it is simply not that confident about its prediction.

Another relationship to Log-Loss is information. The Log-Loss is the cross entropy between the destribution of the true labels and the predictions. By minimizing the cross entropy, we maximize the accuracy of the classifier.

```{r}
mnLogLoss(test_set, lev = levels(test_set$obs))
```
Here the `logLoss` score is pretty good with a value of ~`0.4`. But remember, our data is highly skewed! A perfect model would have a `logLoss=0`.

### AUC (Area Under ROC Curve)

The AUC is an error metric for binary classifiers, while the ROC (Receiver operating characteristic) is a plot that visualizes the performance of a classifier over all possible thresholds. The true positive rate (y-axis) is plotted against the false positive rate (x-axis) as you vary the threshold for assigning observations to a given class.

The AUC is a measure of the model's ability to discriminate between two classes. An area of 1.0 represents a model predicted every test sample correctly. An area of 0.5 means the model is as a good as a fair coin.

How to plot a ROC

```{r, message=FALSE, warning=FALSE}
ggplot(test_set, aes(d = obs, m = Class2)) +
  geom_roc(n.cuts = 5) + style_roc() + 
  geom_rocci(sig.level = .01)
```

[Reference: plotROC documentaion](https://cran.r-project.org/web/packages/plotROC/vignettes/examples.html)

If you simply want a quick summary of the AUC then use caret

```{r}
# Somethings to note:
#    - you can specify which levels to use in the "lev" parameter
#    - so if you want to set the true class to Class2, use:
#      twoClassSummary(test_set, lev = c("Class2", "Class1"))
twoClassSummary(test_set, lev = levels(test_set$obs))
```

This plot shows us that:

- the true positive rate ~0.9 at 0.4 threshold, and a corresponding ~0.15 false positive rate
- the plot looks pretty good because at 0.1 FP, the TP approaches 0.9!
- the numbers on the lines are the thresholds
- the size of the grey boxes are confidence regions for both the Y and X axis at selected thresholds, calculated by the Clopper and Pearson (1934) exact method.

[Reference](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)

### Precision/Recall and F1 Score

Precision and recall are actually two different metrics. But they are often used together. **Precision** answers the question, "Out of the samples that the model predicted to be positive, how many are truly positive?" The **recall** answers the question, "Out of all the samples that are truly positive, how many can be detected by the model?"

![Illustration from Wikipedia:](https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg)

Often you can specify a maximum length `k` for the predictions. This is call called "precision at k" and "recall at k". By varying the value of `k`, the precision and recall vary as well. We can plot the precision vs recall at varying levels of `k`, very much like an ROC curve (explained below). However, we would like to use a single score to reprsent how good a model is based on its precision/recall, and this is the F1 Score.

The F1 Score is a weighted average of the true positive rate (recall) and precision. It is also defined as the harmonic mean between precision and recall:
 
F1 = 2( (precision * recall) / (precision + recall))

Therefore, the F1 score will be small if either precision ore recall is small.

```{r}
# Caret supports Precision/Recall as well
# Note: the AUC reported is the AUC under the Precision/Recall curve, and NOT
# the ROC curve!
prSummary(test_set, lev = levels(test_set$obs))
```

We can get precision recall metrics from the `confusionMatrix()` method as well. With more the values in a confusion matrix, we get a fuller picture.
```{r}
confusionMatrix(data = test_set$pred, reference = test_set$obs, mode = "prec_recall")
```

<!-- =========================================================================== -->

## Regression metrics

Since the output of regression models is continuous, the metrics are different from classifiers. The go to metrics are RMSE and R-squared. Let's go over both!

### Root Mean Squared Error (RMSE)
This is the most typical error metric for regression.
RMSE assumes that the error in the predictions follows a normal distribution, and that there is no overfitting. Intuitively, RMSE is an average over the sum of the delta between the prediction and truth.

$$
RMSE = \sqrt\frac{\sum_{k=1}^n(Predicted_i - Actual_i)^2}{N}
$$

Key points to remember:
- RMSE is the result of squared error, so large deviations from the truth are penalized more
- But because of this, RMSE is susceptible to outliers, especially in smaller datasets
- We don't have to deal with negative errors
- We don't have to deal with absolute values which is used by mean absolute error
- More reliable for larger datasets
- RMSE is a bit harder to interpret than mean absolute error

[For more information and reference](https://en.wikipedia.org/wiki/Root-mean-square_deviation)

### R-squared

R-squared is a measure of how well the model fit to the dataset based on the
predictions and the ground truth. R-squared is greater than or equal to 0 and less than or equal to 1, which represents no fit at all and perfect fit respectively.

Notice that using `RMSE` metric in the caret training function gives us both RMSE and Rsquared results.

[For more information and reference](https://en.wikipedia.org/wiki/Coefficient_of_determination)

### Mean average error (MAE)
MAE measures the average of the errors in a set of predictions, and it avoids negative errors by applying an absolute value. Since there is no square, all errors in the test data are weighted equally.

$$
MAE = \frac{\sum_{k=1}^n  |Predicted_i - Actual_i|}{N}
$$

[For more information and reference](https://en.wikipedia.org/wiki/Mean_absolute_error)

<!-- Get all three with one function! -->
```{r}
postResample(pred = bh_pred, obs = bh_te$medv)
```


# Summary

- the metric you select to optimize your model is important
- the metric you use to evaluate your model is important
- use measure appropriate for **your problem**
- accuracy is often not sufficent/appropriate
- AUC/ROC is pretty popular, but only appropriate for binary classes
- only accuracy generalizes to more than 2 classes!
